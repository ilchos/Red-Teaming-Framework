{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Let's bomb your model!\n",
    "\n",
    "This script bombs your model on our little red-teaming evaluation dataset and saves answers of your model into the file.\n",
    "\n",
    "You can upload this file to our benchmark if you want to get metrics OR you can run the bench.py file to get results yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing\n",
    "\n",
    "You need to set up first things out - load your model.\n",
    "\n",
    "Do it in custom way or use our supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading supported API model\n",
    "\n",
    "Create and place api_keys.json to the repo:\n",
    "`this_repo_folder/config/api_keys.json`\n",
    "\n",
    "api_keys must contain next structure:\n",
    "```json\n",
    "{\n",
    "    \"openai\": {\n",
    "        \"key\": \"YOUR-OPENAI-KEY\"\n",
    "    },\n",
    "    \"langchain\": {\n",
    "        \"key\": \"YOUR-LANGCHAIN-KEY\"\n",
    "    },\n",
    "    \"yandex\": {\n",
    "        \"id\": \"YANDEX-ID\",\n",
    "        \"key\": \"YANDEX-API-KEY\",\n",
    "        \"folder_id\": \"YANDEX-FOLDER-ID\"\n",
    "    },\n",
    "    \"gigachat\": {\n",
    "        \"client_id\": \"GIGACHAT-CLIENT-ID\",\n",
    "        \"secret\": \"GIGACHAT-CLIENT-SECRET\",\n",
    "        \"auth\": \"GIGACHAT-CLIENT-AUTH-CODE\"\n",
    "    },\n",
    "    \"vsegpt\": {\n",
    "        \"base_url\": \"https://api.vsegpt.ru/v1\",\n",
    "        \"key\": \"VSEGPT-API-KEY\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "INSTALL ALL DEPENDS FOR LANGCHAIN, YANDEXGPT, OPENAI, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom LLM\n",
    "For some applications you might want to integrate your model with our framework. Here is the link to a more detailed guide.\n",
    "\n",
    "#### OpenAI-like API\n",
    "Let's start with a simple scenario: when you have an access to your model via an api and it is similar to [OpenAI API](https://platform.openai.com/docs/api-reference/introduction). So the request has the following structure:\n",
    "\n",
    "In this case you simply use Langchain's built-in class `ChatOpenAI` with your custom parameters: `url` and `api_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    base_url=<your url here>,\n",
    "    api_key=<your key here>,\n",
    "    model=<your model here>,\n",
    "    temperature=<your temperature here>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Community Model\n",
    "If there is and implemented python library for the model of your interest then you may use it as well according to the documentation of the package. For example, this is how we implement Yandex models:\n",
    "- install [yandex-chain library](https://github.com/yandex-datasphere/yandex-chain/tree/main)\n",
    "- read their docs on how to load specific LLMs\n",
    "- integrate it to our code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = <load yandex api keys>\n",
    "llm = YandexLLM(\n",
    "    folder_id=keys[\"folder_id\"],\n",
    "    api_key=keys[\"key\"],\n",
    "    model=model,\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Custom API LLM\n",
    "If you want to integrate our framework with a model which has some custom API schema you have to implement it separately. You may take a reference of how to implement it in [yandex-chain](https://github.com/yandex-datasphere/yandex-chain/blob/main/yandex_chain/YandexGPT.py).\n",
    "\n",
    "Here we provide a simple example of defining your model, accsessible via API with custom fields. You need to modify `_convert_messages_to_payload` method to match your api schema $-$ specifically the code in the return statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.schema import (\n",
    "    LLMResult,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from typing import List, Optional\n",
    "import requests\n",
    "\n",
    "\n",
    "class MyLLMApi(LLM):\n",
    "    def __init__(self, api_key: str, api_url: str, **kwargs):\n",
    "        super(LLM).__init__(**kwargs)\n",
    "        self.api_key = api_key\n",
    "        self.api_url = api_url\n",
    "\n",
    "    def _convert_messages_to_payload(self, messages: List[BaseMessage]) -> dict:\n",
    "        # Convert Langchain message objects to your API format\n",
    "        api_messages = []\n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                role = \"user\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                role = \"assistant\"\n",
    "            elif isinstance(message, SystemMessage):\n",
    "                role = \"system\"\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported message type: {type(message)}\")\n",
    "\n",
    "            api_messages.append({\n",
    "                \"role\": role,\n",
    "                \"content\": message.content\n",
    "            })\n",
    "\n",
    "        # Return the converted message payload for your specific API\n",
    "        return {\"messages\": api_messages}\n",
    "\n",
    "    def _call_api(self, payload: dict) -> dict:\n",
    "        # Implement your API request logic here\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
    "        response = requests.post(self.api_url, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "    def _process_api_response(self, response: dict) -> List[AIMessage]:\n",
    "        # Process API response and convert it to AIMessage objects\n",
    "        assistant_message = response['message']\n",
    "        return [AIMessage(content=assistant_message['content'])]\n",
    "\n",
    "    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None) -> LLMResult:\n",
    "        # Convert Langchain messages to your API's request payload\n",
    "        payload = self._convert_messages_to_payload(messages)\n",
    "\n",
    "        # Make the API request\n",
    "        response = self._call_api(payload)\n",
    "\n",
    "        # Process the API response to convert it into Langchain-compatible messages\n",
    "        output_messages = self._process_api_response(response)\n",
    "\n",
    "        # Return LLMResult which contains the generated messages\n",
    "        return LLMResult(generations=[output_messages])\n",
    "\n",
    "    def invoke(self, message: BaseMessage | str) -> BaseMessage:\n",
    "        if isinstance(message, str):\n",
    "            message = HumanMessage(content=message)\n",
    "        result = self._generate([message])\n",
    "        return result.generations[0][0]  # Return the first AIMessage\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom_llm_api\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Let's SCORE your model\n",
    "\n",
    "Scoring your model outputs!\n",
    "\n",
    "Ensure all dependencies is installed & you need to specify & load the judge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python this_repo_folder/benching/bench.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_keys = load_api_keys()\n",
    "\n",
    "loader = LLMLoader()\n",
    "llm = loader.load_vsegpt(\"mistralai/mistral-7b-instruct\", temperature=0.3)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
