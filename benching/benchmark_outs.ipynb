{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Let's bomb your model!\n",
    "\n",
    "This script bombs your model on our little red-teaming evaluation dataset and saves answers of your model into the file.\n",
    "\n",
    "You can upload this file to our benchmark if you want to get metrics OR you can run the bench.py file to get results yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing\n",
    "\n",
    "You need to set up first things out - load your model.\n",
    "\n",
    "Do it in custom way or use our supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading supported API model\n",
    "\n",
    "Create and place api_keys.json to the repo:\n",
    "`this_repo_folder/config/api_keys.json`\n",
    "\n",
    "api_keys must contain next structure:\n",
    "```json\n",
    "{\n",
    "    \"openai\": {\n",
    "        \"key\": \"YOUR-OPENAI-KEY\"\n",
    "    },\n",
    "    \"langchain\": {\n",
    "        \"key\": \"YOUR-LANGCHAIN-KEY\"\n",
    "    },\n",
    "    \"yandex\": {\n",
    "        \"id\": \"YANDEX-ID\",\n",
    "        \"key\": \"YANDEX-API-KEY\",\n",
    "        \"folder_id\": \"YANDEX-FOLDER-ID\"\n",
    "    },\n",
    "    \"gigachat\": {\n",
    "        \"client_id\": \"GIGACHAT-CLIENT-ID\",\n",
    "        \"secret\": \"GIGACHAT-CLIENT-SECRET\",\n",
    "        \"auth\": \"GIGACHAT-CLIENT-AUTH-CODE\"\n",
    "    },\n",
    "    \"vsegpt\": {\n",
    "        \"base_url\": \"https://api.vsegpt.ru/v1\",\n",
    "        \"key\": \"VSEGPT-API-KEY\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "INSTALL ALL DEPENDS FOR LANGCHAIN, YANDEXGPT, OPENAI, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading custom model\n",
    "\n",
    "If you use your custom model, just provide it to the this \"generate\" function:\n",
    "\n",
    "```python\n",
    "def generate(system_prompt: str, user_prompt: str) -> str:\n",
    "\n",
    "    # your function initialization, in example:\n",
    "    # return model.generate(f\"\"\"system:\n",
    "\n",
    "{system_prompt}\n",
    "\n",
    "user:\n",
    "\n",
    "{user_prompt}\n",
    "\n",
    "assistant: \"\"\")\n",
    "```\n",
    "\n",
    "Otherwise, use our\n",
    "```\n",
    "import generate from benching\n",
    "```\n",
    "\n",
    "INSTALL ONLY pandas / other little things needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO load benchmark data\n",
    "# TODO generate answers on data\n",
    "# TODO save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Let's SCORE your model\n",
    "\n",
    "Scoring your model outputs!\n",
    "\n",
    "Ensure all dependencies is installed & you need to specify & load the judge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python this_repo_folder/benching/bench.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
